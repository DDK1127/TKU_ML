{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "|details-start|\n",
      "**References**\n",
      "|details-split|\n",
      "\n",
      "- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "  Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "  Structure and Classification Rule for Recognition in Partially Exposed\n",
      "  Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "  Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "  on Information Theory, May 1972, 431-433.\n",
      "- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "  conceptual clustering system finds 3 classes in the data.\n",
      "- Many, many more ...\n",
      "\n",
      "|details-end|\n",
      "[0 0 0 0 0]\n",
      "\n",
      "Class labels [0 1 2]\n",
      "\n",
      "X_train shape: (105, 2)\n",
      "\n",
      "X_test shape: (45, 2)\n",
      "\n",
      "y_train shape: (105,)\n",
      "\n",
      "y_test shape: (45,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# 載入Iris資料集\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "print(iris.DESCR) # 檢查一下品種是否正確？\n",
    "print(iris.target[:5])\n",
    "\n",
    "X = iris.data[:, [2, 3]]  # 花瓣長度(petal length)與花瓣寬度(petal width)\n",
    "y = iris.target\n",
    "\n",
    "# 將test資料分成 3:7, =>test_size = 0.3, 剩下 0.7 為train資料\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print('\\nClass labels', np.unique(y))\n",
    "print(\"\\nX_train shape:\", X_train.shape)\n",
    "print(\"\\nX_test shape:\", X_test.shape)\n",
    "print(\"\\ny_train shape:\", y_train.shape)\n",
    "print(\"\\ny_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_std shape: (105, 2)\n",
      "X_test_std shape: (45, 2)\n",
      "Mean =  -3.5738607840314564e-16\n",
      "\n",
      "Standard deviation =  0.9999999999999999\n",
      "\n",
      "first 10 stdandardized sample => \n",
      " [[-0.18295039 -0.29318114]\n",
      " [ 0.93066067  0.7372463 ]\n",
      " [ 1.04202177  1.63887031]\n",
      " [ 0.6522579   0.35083601]\n",
      " [ 1.09770233  0.7372463 ]\n",
      " [ 0.03977182 -0.16437771]\n",
      " [ 1.26474398  1.38126345]\n",
      " [ 0.48521625  0.47963944]\n",
      " [-0.01590873 -0.16437771]\n",
      " [ 0.59657735  0.7372463 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 標準化資料\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "# 合併標準化後的訓練和測試資料\n",
    "X_combined_std = np.vstack((X_train_std, X_test_std))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "\n",
    "print(\"X_train_std shape:\", X_train_std.shape)\n",
    "print(\"X_test_std shape:\", X_test_std.shape)\n",
    "\n",
    "print('Mean = ', X_train_std[:,0].mean())\n",
    "print('\\nStandard deviation = ', X_train_std[:,0].std())\n",
    "print('\\nfirst 10 stdandardized sample => \\n', X_train_std[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_region(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        marker = markers[idx]\n",
    "        if marker == 'x':\n",
    "            plt.scatter(x=X[y == cl, 0], \n",
    "                        y=X[y == cl, 1],\n",
    "                        alpha=0.8, \n",
    "                        c=colors[idx],\n",
    "                        marker=marker, \n",
    "                        label=cl)\n",
    "        else:\n",
    "            plt.scatter(x=X[y == cl, 0], \n",
    "                        y=X[y == cl, 1],\n",
    "                        alpha=0.8, \n",
    "                        c=colors[idx],\n",
    "                        marker=marker, \n",
    "                        label=cl, \n",
    "                        edgecolor='black')\n",
    "\n",
    "    if test_idx:\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1], c='none', edgecolor='black', alpha=1.0,\n",
    "                    linewidth=1, marker='o', s=100, label='test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入必要的套件\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# 訓練和評估六種不同的機器學習演算法\n",
    "classifiers = {\n",
    "    # n_iter is removed from version 0.21, so I use max_iter to replace it.\n",
    "    'Perceptron': Perceptron(max_iter=40, eta0=0.1, random_state=0),\n",
    "    'Logistic Regression': LogisticRegression(C=100.0, random_state=0),\n",
    "    'SVM': SVC(kernel='linear', C=1.0, random_state=0),\n",
    "    'Decision Tree': DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0),\n",
    "    # Max_depth 可控制樹的高度，避免太深造成過擬和\n",
    "    'Random Forest': RandomForestClassifier(criterion='gini', n_estimators=50, random_state=0, n_jobs=2),\n",
    "    # n_estimators 決策樹的數量，越大越能提高準確率，不過計算量也會增加\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_std, y_train)\n",
    "    \n",
    "    # 訓練準確率\n",
    "    y_train_pred = clf.predict(X_train_std)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    \n",
    "    # 測試準確率\n",
    "    y_test_pred = clf.predict(X_test_std)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    # 其他評估指標\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_test_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "    \n",
    "    results.append((name, train_accuracy, test_accuracy, precision, recall, f1))\n",
    "    \n",
    "    print(f\"{name} - Train Accuracy: {train_accuracy:.3f}, Test Accuracy: {test_accuracy:.3f}\")\n",
    "    \n",
    "    plot_decision_region(X_combined_std, y_combined, classifier=clf, test_idx=range(len(y_train), len(y_combined)))\n",
    "    plt.title(name)\n",
    "    plt.xlabel('Petal length [standardized]')\n",
    "    plt.ylabel('Petal width [standardized]')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of accuracies:\n",
      "Perceptron: Train Accuracy=0.924, Test Accuracy=0.889, Precision=0.895, Recall=0.896, F1 Score=0.891\n",
      "Logistic Regression: Train Accuracy=0.952, Test Accuracy=0.978, Precision=0.972, Recall=0.981, F1 Score=0.976\n",
      "SVM: Train Accuracy=0.952, Test Accuracy=0.978, Precision=0.972, Recall=0.981, F1 Score=0.976\n",
      "Decision Tree: Train Accuracy=0.981, Test Accuracy=0.978, Precision=0.972, Recall=0.981, F1 Score=0.976\n",
      "Random Forest: Train Accuracy=0.990, Test Accuracy=0.978, Precision=0.972, Recall=0.981, F1 Score=0.976\n",
      "KNN: Train Accuracy=0.952, Test Accuracy=1.000, Precision=1.000, Recall=1.000, F1 Score=1.000\n",
      "             Algorithm     訓練準確度     測試準確度  Precision    Recall  F1 Score\n",
      "0           Perceptron  0.923810  0.888889   0.894843  0.895623  0.890620\n",
      "1  Logistic Regression  0.952381  0.977778   0.972222  0.981481  0.975983\n",
      "2                  SVM  0.952381  0.977778   0.972222  0.981481  0.975983\n",
      "3        Decision Tree  0.980952  0.977778   0.972222  0.981481  0.975983\n",
      "4        Random Forest  0.990476  0.977778   0.972222  0.981481  0.975983\n",
      "5                  KNN  0.952381  1.000000   1.000000  1.000000  1.000000\n"
     ]
    }
   ],
   "source": [
    "# 顯示結果\n",
    "print(\"Summary of accuracies:\")\n",
    "for name, train_accuracy, test_accuracy, precision, recall, f1 in results:\n",
    "    print(f\"{name}: Train Accuracy={train_accuracy:.3f}, Test Accuracy={test_accuracy:.3f}, Precision={precision:.3f}, Recall={recall:.3f}, F1 Score={f1:.3f}\")\n",
    "\n",
    "# 將結果整理成表格\n",
    "results_df = pd.DataFrame(results, columns=['Algorithm', '訓練準確度', '測試準確度', 'Precision', 'Recall', 'F1 Score'])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores for KNN: [0.85714286 1.         1.         0.95238095 0.95238095]\n",
      "Mean cross-validation score for KNN: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# KNN => 100%? \n",
    "# 使用交叉驗證來評估KNN模型 \n",
    "knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n",
    "scores = cross_val_score(knn, X_train_std, y_train, cv=5)\n",
    "print(f\"Cross-validation scores for KNN: {scores}\")\n",
    "print(f\"Mean cross-validation score for KNN: {scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 個人小小結論\n",
    "    KNN的訓練和測試準確率都達到100%，有可能是因爲Iris的資料集較小且容易區分，且同時資料的分佈又是線性分佈，所以才會達到如此好的結果，再者，我怕KNN是直接記住訓練資料，而如果test data 與 train data非常相似的話，很有可能是過度擬和造成，導致可能在其他資料集不會有如此好的表現，因此我再多做交叉驗證的方法，得知該模型確實有不錯的表現，應為資料集的緣故？。\n",
    "    \n",
    "    Logistic Regression、SVM 和 Decision Tree 都有接近的表現，準確率和其他指標都在 0.97 左右。這顯示這些模型也非常適合該資料集\n",
    "    同時我也將Decision Tree的n_estimators提高到50以達到更好的效果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
